{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsdYE7/TxknGw0DyX0VqgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Textual-Data-Analysis-25/blob/main/Exercise%20task%2010.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL8mj0GXJXzb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is simple: read the following paper: Revisiting Relation Extraction in the era of Large Language Models and answer the following questions.\n",
        "\n",
        "Please note that in this paper, the authors have used \"relation extraction\" term to refer to joint extraction of entities (NER) and associations between them. This is done by some authors. However, in the lecture we taught that this is more broadly known as \"end-to-end\" information extraction.     \n",
        "\n",
        "Now, answer the following questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "CR91-kHcJbfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Be honest! Did you find this paper interesting or boring? Why? (Your feedback will be used to improve the course)\n",
        "\n",
        "Generally, the article looks fancy as it simply explain what they have done and shortly having extra information and prompts in the appendix. As the article is short and simply explained, I could easilly read it and know how they improve the model and also learned about one of th egood model as Flan-T5. So , generally the presented data is useful and to know how chain of though and prompting could help to improve the results."
      ],
      "metadata": {
        "id": "B_3Lv3jNJcyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In a few sentences describe what the paper is all about, what are the main contributions of the authors, and what are the methods, findings and the results.\n",
        "\n",
        "Fine-tuning, few-shot prompting and learning, and using a chain of thought could improve th emodel significantly to achieve the state of the art performance. MOreover, exact matching or strict evaluation is not realistic leading to the better evaluation by manual annotation. They also indicate that LLMs should be a standard baseline for RE"
      ],
      "metadata": {
        "id": "Z-vTPD4qJd9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why evaluating  end-to-end IE systems that are based on seq-2-seq approach (text generation) is tricky? How have the authors dealt with this problem?\n",
        "\n",
        "Traditional evaluation requires exact match between generated text and base text which is hard to achieve this strict amtch. This may be possibel if th emodel is fine-tuned trained on the very large dataset to make the model to generate standard output. However, generative model like GPT could generate text with the similar meaning but not hte same or exact match with refrence text. Or even if the output is the same, it could be in different format and shape of presentation.\n",
        "The authors address this issue by evaluating the model using the manually decision making to see if the snetence convey th esame meaning or not. Furthermore, making output together with chain of thought excplanaition helps to improve the results."
      ],
      "metadata": {
        "id": "_AYtsAL8Jeyv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMSu1uJ1ubuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}