{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPro1i1JoDlvpU0F5vSmcZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Textual-Data-Analysis-25/blob/main/Exercise%20task%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI-yNJk-B1_8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web crawl-based corpora\n",
        "Read the paper The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale (Penedo et al. 2024), which introduces the FineWeb and FineWeb-Edu datasets (version 1). Answer the following questions:\n",
        "\n",
        "What are the key processing steps to create the FineWeb data from crawl sources, and what is the most important tool or method to implement each?\n",
        "\n",
        "Text extraction (using the open source trafilatura library), Base filtering (basic filtering pipeline, URL filtering using a blocklist and quality and repetition filters from MassiveText), Deduplication (with MinHash, a fuzzy hash-based deduplication technique), Adding C4â€™s filters (base filtering and independent MinHash), Developing additional heuristic filters (developed heuristic filters through data inspection), and Personal Identifiable Information (PII) removal (by anonymizing email and public IP addresses) for public use\n",
        "\n",
        "\n",
        "\n",
        "Does the processing to create the FineWeb data omit any of the processing steps discussed on the lecture, and does it add any? (what?)\n",
        "\n",
        "Other filtaration could be used or tested to examine the results. Some legal and identification issues may need to be addressed in more details.\n",
        "\n",
        "\n",
        "\n",
        "What are the key differences between the FineWeb and FineWeb-edu datasets, and what are the key steps to create the latter from the former?\n",
        "The former one containg the general available text on th einternet and webs while th elatter one is more filtered and pullished educationally from synthetic annotations generated by Llama-3- 70B-Instruct to be bale to use data appropriate for educaitonal institutes.\n",
        "Tehrefore, to achieve that, synthetic annotations is generated by Llama-3- 70B-Instruct for data filtering purposes. Linear regresison model is trained to be used on FinWeb on vairous scales. The results lead to the higher score of MMLU, which increases from 33% to 37%, a relative improvement of approximately 12%, and ARC score goes from 46% to 57%, an improvement of about 24%.\n"
      ],
      "metadata": {
        "id": "fQ1eB-RYCHs2"
      }
    }
  ]
}