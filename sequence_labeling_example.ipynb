{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Textual-Data-Analysis-25/blob/main/sequence_labeling_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence labeling example\n",
        "\n",
        "Let's train a transformer model on a Named Entity Recognition (NER) dataset."
      ],
      "metadata": {
        "id": "6z3NuDfrprZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setup"
      ],
      "metadata": {
        "id": "adKOlw8frrlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required Python packages:"
      ],
      "metadata": {
        "id": "Ya7aNGYeqcql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t5cMszypelm"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet fsspec==2024.10.0 transformers datasets evaluate seqeval\n",
        "pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the libraries we'll be using here."
      ],
      "metadata": {
        "id": "UhlAurZOvpWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import evaluate\n",
        "\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "h19T7mGuvt8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make things a bit more quiet. (This only affects what shows on screen when running. If you're debugging, you probably want to comment these out.)"
      ],
      "metadata": {
        "id": "LySPAWdSqjA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.utils.logging.set_verbosity_error()\n",
        "datasets.logging.set_verbosity_error()\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "uo-wMrnKqpIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Load dataset\n",
        "\n",
        "Load a dataset for training using `datasets`."
      ],
      "metadata": {
        "id": "kUKGKvnHqKt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'conll2003'\n",
        "\n",
        "builder = datasets.load_dataset_builder(DATASET)\n",
        "dataset = datasets.load_dataset(DATASET)"
      ],
      "metadata": {
        "id": "3lZDBHc7ppI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "9cfab584-e982-43a5-e00c-0f35408d74c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2e2ba876877d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     )\n\u001b[0;32m-> 2149\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m     \u001b[0;31m# Rename and cast features to match task schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0mis_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_remote_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             raise FileNotFoundError(\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the description and dataset."
      ],
      "metadata": {
        "id": "fzvOrvMUrQW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(builder.info.description)"
      ],
      "metadata": {
        "id": "7PUGnArpq0I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "RY-ER5vErVEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the conventional split into `train`, `validation`, and `test`.\n",
        "\n",
        "We're here only interested in the `tokens` and `ner_tags`. (In particular, the `ner_tags` and `chunk_tags` are included to support methods based on manually engineered features, and as such not highly relevant to the deep learning approach we're pursuing here.)\n",
        "\n",
        "Let's have a look at one example."
      ],
      "metadata": {
        "id": "wK0I81MzWfW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0]['tokens'])\n",
        "print(dataset['train'][0]['ner_tags'])"
      ],
      "metadata": {
        "id": "xg7YVgnCXglr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the number of different labels and create mappings from label IDs to label strings and vice versa; we'll need these later."
      ],
      "metadata": {
        "id": "6DlQnP8N7EyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = dataset['train'].features['ner_tags'].feature.names\n",
        "print('Labels:', label_names)\n",
        "\n",
        "num_labels = len(label_names)\n",
        "id2label = { k: v for k, v in enumerate(label_names) }\n",
        "label2id = { v: k for k, v in enumerate(label_names) }\n",
        "\n",
        "print('Number of labels:', num_labels)\n",
        "print('id2label mapping:', id2label)\n",
        "print('labelid2 mapping:', label2id)"
      ],
      "metadata": {
        "id": "JHGjHsNf7I4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see that example again, applying the label mapping:"
      ],
      "metadata": {
        "id": "fvxCFhkAYMXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token, tag_id in zip(dataset['train'][0]['tokens'], dataset['train'][0]['ner_tags']):\n",
        "    print(f'{token}\\t{id2label[tag_id]}')"
      ],
      "metadata": {
        "id": "GPqVWzNyYSFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Tokenize and vectorize dataset\n",
        "\n",
        "As in the [text classification notebook](https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_basic_example.ipynb), we'll first load the tokenizer that corresponds to the model that we want to use. `AutoTokenizer` is a convenience class that will return the appropriate tokenizer for the model it's given as an argument:"
      ],
      "metadata": {
        "id": "d2yaMXe7unE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'bert-base-cased'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "IG2DtYoRvGWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer will, most importantly, produce `input_ids`, which identify the tokens of the text.\n",
        "\n",
        "The BERT tokenizer also produces an `attention_mask`, which can be used to make the model ignore some tokens, and `token_type_ids`, which can differentiate parts of the input e.g. when it consists of two separate texts."
      ],
      "metadata": {
        "id": "yI3Ve7f3vfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer('this is an example sentence'))"
      ],
      "metadata": {
        "id": "b7sIEg4ivgoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key point here is to note that the data already has its own definition of \"token\", and the tokenizer may split some of those into parts:"
      ],
      "metadata": {
        "id": "Cz-TQwRsbR7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer('Turku is not in the vocabulary').input_ids\n",
        "\n",
        "\n",
        "print(input_ids)\n",
        "print()\n",
        "print(tokenizer.convert_ids_to_tokens(input_ids))"
      ],
      "metadata": {
        "id": "UlJOXt_1a3oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sentence, our data consists of a tokenized list of strings (\"words\") rather than a single string. If we call the tokenizer with its default options, it interprets each token as a different example:"
      ],
      "metadata": {
        "id": "8pflRkNAeJD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0]['tokens'])\n",
        "print()\n",
        "\n",
        "for ids in tokenizer(dataset['train'][0]['tokens']).input_ids:\n",
        "    print(tokenizer.convert_ids_to_tokens(ids))"
      ],
      "metadata": {
        "id": "KLHE2uUWeNob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the correct mapping, we provide `is_split_into_words=True` to the tokenizer."
      ],
      "metadata": {
        "id": "Qg0gitvecGDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = 'Turku is not in the vocabulary'.split()\n",
        "tokenized = tokenizer(tokens, is_split_into_words=True)\n",
        "\n",
        "print(tokens)\n",
        "print()\n",
        "pprint(tokenizer.convert_ids_to_tokens(tokenized.input_ids))"
      ],
      "metadata": {
        "id": "cVY4p2I3cg6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer also provides us with a mapping from the tokenizer tokens to \"original\" tokens (\"words\")"
      ],
      "metadata": {
        "id": "p_YMxebffxow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized.word_ids()"
      ],
      "metadata": {
        "id": "QbdQNIcIf6qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `is_split_into_words=True`, we can tokenize the input so that its tokens are compatible with the model, but the labels will be misaligned."
      ],
      "metadata": {
        "id": "yyKqaqW2xoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "\n",
        "token_ids = tokenizer(dataset['train'][0]['tokens'], is_split_into_words=True).input_ids\n",
        "tag_ids = dataset['train'][0]['ner_tags']\n",
        "\n",
        "for token_id, tag_id in zip_longest(token_ids, tag_ids):\n",
        "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "    tag = id2label[tag_id] if tag_id is not None else None\n",
        "    print(f'{token}\\t{tag}')"
      ],
      "metadata": {
        "id": "JKC8KstnyEkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To resolve this, we'll borrow a function from [a transformers tutorial](https://huggingface.co/course/chapter7/2). Here, `-100` is a \"magic value\" for a label that pytorch ignores."
      ],
      "metadata": {
        "id": "HsmDsthgzGoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:    # Start of a new word\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:          # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:                          # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            if label % 2 == 1:         # If label is B-XXX we change it to I-XXX\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "S_cxn0BIzS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also borrow a function for jointly tokenizing the text and aliging labels:"
      ],
      "metadata": {
        "id": "R8nx5IKK0A5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(inputs):\n",
        "    outputs = tokenizer(inputs['tokens'], truncation=True, is_split_into_words=True)\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(inputs['ner_tags']):\n",
        "        word_ids = outputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "    outputs['labels'] = new_labels\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "rzQ1o5103usD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then apply this to the whole dataset:"
      ],
      "metadata": {
        "id": "TljzveA14amt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "6MQVEP6y4Z-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now these should match up:"
      ],
      "metadata": {
        "id": "YU_kJtqu7It2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = dataset['train'][0]['input_ids']\n",
        "tag_ids = dataset['train'][0]['labels']\n",
        "\n",
        "for token_id, tag_id in zip_longest(token_ids, tag_ids):\n",
        "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "    tag = id2label[tag_id] if tag_id != -100 else None\n",
        "    print(f'{token}\\t{tag}')"
      ],
      "metadata": {
        "id": "wgRVPp6p7NjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Instantiate model\n",
        "\n",
        "Now, we'll instantiate a pretrained model with a sequence labeling head. In the `transformers` library, this class of models are named `...ModelForTokenClassification`. (cf. `...ModelForSequenceClassification`) We'll again use the `Auto` variant to get the appropriate class based on model name.\n",
        "\n",
        "**NOTE**: we need to provide the number of labels to `from_pretrained` so that the function knows the size of the output layer that is required. The `id2label` and `label2id` mappings allow the model to report its classification results in interpretable text labels."
      ],
      "metadata": {
        "id": "coct30Pmr0Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "m5D1eK0FraRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Training configuration\n",
        "\n",
        "To assess the progress and results of training, we'll use the standard `seqeval` library. We'll also need to introduce a function that takes model outputs and the labels from the dataset and calls the metric.\n",
        "\n",
        "Here, we'll again borrow from [the transformers tutorial](https://huggingface.co/course/chapter7/2):"
      ],
      "metadata": {
        "id": "GglUCNFm1f9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate.load('seqeval')\n",
        "\n",
        "\n",
        "def compute_metrics(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = outputs.argmax(axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[id2label[i] for i in label if i != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metrics.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        'precision': all_metrics['overall_precision'],\n",
        "        'recall': all_metrics['overall_recall'],\n",
        "        'f1': all_metrics['overall_f1'],\n",
        "        'accuracy': all_metrics['overall_accuracy'],\n",
        "    }"
      ],
      "metadata": {
        "id": "Q1ph62MmsTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need a collator for padding the examples to the same length to process them in batches."
      ],
      "metadata": {
        "id": "1cDpsGI_-2hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Hp9Cwl8X-6KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TrainingArguments` class configures many of the details of the model training. You may want to try optimizing the following hyperparameters to improve model performance:\n",
        "\n",
        "* `learning_rate`: the step size for weight updates\n",
        "* `per_device_train_batch_size`: number of examples per training batch\n",
        "* `max_steps`: the maximum number of steps to train for"
      ],
      "metadata": {
        "id": "5CMOftQi4nIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = transformers.TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir='checkpoints',\n",
        "    evaluation_strategy='steps',\n",
        "    logging_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    learning_rate=0.00002,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    max_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "T13HyLrh2oRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll create a custom [callback](https://huggingface.co/docs/transformers/main_classes/callback) to store values logged during training so that we can more easily examine them later. (This is only needed for visualization and is not necessary to understand in detail.)"
      ],
      "metadata": {
        "id": "q-er8P2eAHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class LogSavingCallback(transformers.TrainerCallback):\n",
        "    def on_train_begin(self, *args, **kwargs):\n",
        "        self.logs = defaultdict(list)\n",
        "        self.training = True\n",
        "\n",
        "    def on_train_end(self, *args, **kwargs):\n",
        "        self.training = False\n",
        "\n",
        "    def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
        "        if self.training:\n",
        "            for k, v in logs.items():\n",
        "                if k != \"epoch\" or v not in self.logs[k]:\n",
        "                    self.logs[k].append(v)\n",
        "\n",
        "training_logs = LogSavingCallback()"
      ],
      "metadata": {
        "id": "DwMxldxOAUHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Train (fine-tune) model"
      ],
      "metadata": {
        "id": "enKZualc5hzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[training_logs],\n",
        ")"
      ],
      "metadata": {
        "id": "3rauYFl45mi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "rTCrH63X50T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Evaluate trained model\n",
        "\n",
        "We can use the `trainer` to evaluate the trained model using the metric we defined:"
      ],
      "metadata": {
        "id": "SY4dt3HGA-6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dataset['test'])\n",
        "\n",
        "pprint(eval_results)\n",
        "\n",
        "print('\\nF1:', eval_results['eval_f1'])"
      ],
      "metadata": {
        "id": "W3VOmny26WTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we captured performance during training using the `training_logs` callback, we can also have a look at training and evaluation loss and evaluation $F_1$ progression. (The code here is only for visualization and you do not need to understand it, but you should aim to be able to interpret the plots.)"
      ],
      "metadata": {
        "id": "1jxFdf0uBXYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(logs, keys, labels):\n",
        "    values = sum([logs[k] for k in keys], [])\n",
        "    plt.ylim(max(min(values)-0.1, 0.0), min(max(values)+0.1, 1.0))\n",
        "    for key, label in zip(keys, labels):\n",
        "        plt.plot(logs['epoch'], logs[key], label=label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot(training_logs.logs, ['loss', 'eval_loss'], ['Training loss', 'Evaluation loss'])"
      ],
      "metadata": {
        "id": "DWLWZBjABEc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(training_logs.logs, ['eval_f1'], ['Evaluation F1'])"
      ],
      "metadata": {
        "id": "8M1IYKS7Bp6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Create pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "PWTkJ97rDq7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can wrap our fine-tuned model in a pipeline for convenience. (We need to specify `device` here as the model is on GPU.)"
      ],
      "metadata": {
        "id": "BVlTovCFEgLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = transformers.pipeline(\n",
        "    'token-classification',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    aggregation_strategy='simple',\n",
        "    device=0\n",
        ")"
      ],
      "metadata": {
        "id": "pbw6bQL9Dto0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the pipeline simply as follows:"
      ],
      "metadata": {
        "id": "kxSAAtmAEuAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe('Finnish cities include Turku and Tampere.')"
      ],
      "metadata": {
        "id": "nTOhvnJ6EIww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, for convenience"
      ],
      "metadata": {
        "id": "Tl1a9_JHIbGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tag(text):\n",
        "    output = pipe(text)\n",
        "    print('input:', text)\n",
        "    print('output:', [(o['word'], o['entity_group']) for o in output])\n",
        "\n",
        "tag('Finnish cities include Turku and Tampere.')"
      ],
      "metadata": {
        "id": "XsjFMSRwIPye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag('Paavo Nurmi was born in Turku in 1897.')"
      ],
      "metadata": {
        "id": "eI0QzsQ7I7_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag('Nokia is a company founded near the town of Nokia.')"
      ],
      "metadata": {
        "id": "cqJP87L7JcbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}