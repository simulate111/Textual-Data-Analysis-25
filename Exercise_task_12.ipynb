{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHg0Mz/xRQWzfRiiDSym99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Textual-Data-Analysis-25/blob/main/Exercise_task_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning :\n",
        "https://arxiv.org/pdf/2501.12948"
      ],
      "metadata": {
        "id": "LcvZAn8iFxKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- The work involves data that separates \"thinking\" tokens (<think>...</think>) from answer tokens (<answer>...</answer>). Where does this data come from?\n",
        "\n",
        "\n",
        "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
        "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:\n",
        "\n",
        "\n",
        "Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.\n",
        "\n",
        "DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach."
      ],
      "metadata": {
        "id": "AstwZzF3FniK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- The process of creating the models involves the following techniques. Briefly define each and its primary role in the process:\n",
        "The reward is the source of the training signal, which decides the optimization direction of RL.\n",
        "Reward Modeling and Accuracy vs. format rewards:\n",
        "• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\n",
        "For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
        "• Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\n",
        "tags.\n",
        "\n",
        "Rejection sampling:\n",
        "A technique used to improve the training dataset by sampling multiple responses from the model and selecting only the highest-quality outputs.\n",
        "By curating reasoning prompts and generating reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training.\n",
        "\n",
        "Supervised fine-tuning:\n",
        "A process where a model is fine-tuned on high-quality labeled data to improve its performance.\n",
        "When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks.\n",
        "\n",
        "Distillation:\n",
        "A process where a model is fine-tuned on high-quality labeled data to improve its performance.\n",
        "We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s68veKLQFrKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3- The authors use and introduce a number of different models. Briefly define the relationships between the following pairs of models and how information flows between them, relating this to the methods above when relevant:\n",
        "\n",
        "DeepSeek-V3-Base and DeepSeek-R1-Zero:\n",
        "DeepSeek-R1-Zero is trained directly from DeepSeek-V3-Base using reinforcement learning (RL) without any initial supervised fine-tuning. This allows the model to develop reasoning capabilities purely through RL​\n",
        "\n",
        "DeepSeek-V3-Base and DeepSeek-R1:\n",
        "DeepSeek-R1 starts with DeepSeek-V3-Base but undergoes cold-start fine-tuning on curated reasoning data before RL. This helps address issues in DeepSeek-R1-Zero, such as readability and language mixing​\n",
        "\n",
        "DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70B:\n",
        "DeepSeek-R1-Distill-Llama-70B is a distilled version of DeepSeek-R1, meaning that its knowledge has been transferred to a smaller Llama-70B model through fine-tuning. This process helps smaller models retain DeepSeek-R1’s reasoning capabilities​"
      ],
      "metadata": {
        "id": "-2_anfSlFtD1"
      }
    }
  ]
}